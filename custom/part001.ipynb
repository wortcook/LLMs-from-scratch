{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  source_language target_lang  \\\n",
      "0              en          de   \n",
      "1              en          de   \n",
      "2              en          de   \n",
      "3              en          de   \n",
      "4              en          de   \n",
      "\n",
      "                                                data  \n",
      "0  {'source': 'I think that she won't come.', 'ta...  \n",
      "1  {'source': 'I often have ear infections.', 'ta...  \n",
      "2  {'source': 'You're correct.', 'target': 'Sie h...  \n",
      "3  {'source': 'Your opinion is quite different fr...  \n",
      "4  {'source': 'A lot of new buildings have gone u...  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import io\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "DE_TEST_FILE  = 'deu_test.json'\n",
    "DE_TRAIN_FILE = 'deu_training.json'\n",
    "FR_TEST_FILE  = 'fra_test.json'\n",
    "FR_TRAIN_FILE = 'fra_training.json'\n",
    "ES_TEST_FILE  = 'spa_test.json'\n",
    "ES_TRAIN_FILE = 'spa_training.json'\n",
    "\n",
    "def read_json_from_git(fname):\n",
    "    url = f'https://raw.githubusercontent.com/wortcook/LLMs-from-scratch/refs/heads/main/data/{fname}'\n",
    "    s = requests.get(url).content\n",
    "    \n",
    "    return pd.read_json(io.StringIO(s.decode('utf-8')))\n",
    "\n",
    "de_test  = read_json_from_git(DE_TEST_FILE)\n",
    "print(de_test.head())\n",
    "\n",
    "\n",
    "#tokenize\n",
    "def tokenize(sentence):\n",
    "    return sentence.split()\n",
    "\n",
    "vocab = []\n",
    "\n",
    "#def add to vocab\n",
    "def add_to_vocab(word):\n",
    "    if word not in vocab:\n",
    "        vocab.append(word)\n",
    "    return vocab.index(word)\n",
    "\n",
    "#iterating over the rows of the dataframe\n",
    "for index, row in de_test.iterrows():\n",
    "    row_tokens = tokenize(row['source'])\n",
    "    \n",
    "    token_ids = []\n",
    "    \n",
    "    for token in row_tokens:\n",
    "        token_ids.append(add_to_vocab(token))\n",
    "        \n",
    "    de_test.at[index, 'source_tokens'] = row_tokens\n",
    "    de_test.at[index, 'source_token_ids'] = token_ids\n",
    "    \n",
    "print(de_test.head())\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmscratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

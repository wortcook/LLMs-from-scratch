{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************************************************\n",
      "101\n",
      "76\n",
      "********************************************************************************\n",
      "shape: (5, 6)\n",
      "┌────────────────┬────────────────┬────────────────┬───────────────┬───────────────┬───────────────┐\n",
      "│ source         ┆ target         ┆ source_tokens  ┆ target_tokens ┆ source_indice ┆ target_indice │\n",
      "│ ---            ┆ ---            ┆ ---            ┆ ---           ┆ s             ┆ s             │\n",
      "│ str            ┆ str            ┆ list[str]      ┆ list[str]     ┆ ---           ┆ ---           │\n",
      "│                ┆                ┆                ┆               ┆ list[u16]     ┆ list[u16]     │\n",
      "╞════════════════╪════════════════╪════════════════╪═══════════════╪═══════════════╪═══════════════╡\n",
      "│ Tom didn't     ┆ Tom hat nicht  ┆ [\"Tom\",        ┆ [\"Tom\",       ┆ [1, 2, … 4]   ┆ [1, 2, … 5]   │\n",
      "│ even smile.    ┆ mal gelächelt. ┆ \"didn't\", …    ┆ \"hat\", …      ┆               ┆               │\n",
      "│                ┆                ┆ \"smile.\"]      ┆ \"gelächelt.\"] ┆               ┆               │\n",
      "│ Tom is a       ┆ Tom ist ein    ┆ [\"Tom\", \"is\",  ┆ [\"Tom\",       ┆ [1, 5, … 9]   ┆ [1, 6, … 10]  │\n",
      "│ really bad     ┆ wirklich       ┆ … \"singer.\"]   ┆ \"ist\", …      ┆               ┆               │\n",
      "│ singer.        ┆ schlechte…     ┆                ┆ \"Sänger.\"]    ┆               ┆               │\n",
      "│ I don't want   ┆ Ich will nicht ┆ [\"I\", \"don't\", ┆ [\"Ich\",       ┆ [10, 11, …    ┆ [11, 12, …    │\n",
      "│ to fail my     ┆ durch meine    ┆ … \"exams.\"]    ┆ \"will\", …     ┆ 16]           ┆ 16]           │\n",
      "│ exams.         ┆ Prü…           ┆                ┆ \"fallen.\"]    ┆               ┆               │\n",
      "│ There were     ┆ Es gab viele   ┆ [\"There\",      ┆ [\"Es\", \"gab\", ┆ [17, 18, …    ┆ [17, 18, …    │\n",
      "│ many wounded.  ┆ Verletzte.     ┆ \"were\", …      ┆ …             ┆ 20]           ┆ 20]           │\n",
      "│                ┆                ┆ \"wounded.\"…    ┆ \"Verletzte.\"] ┆               ┆               │\n",
      "│ We became very ┆ Wir wurden     ┆ [\"We\",         ┆ [\"Wir\",       ┆ [21, 22, …    ┆ [21, 22, …    │\n",
      "│ good friends.  ┆ sehr gute      ┆ \"became\", …    ┆ \"wurden\", …   ┆ 25]           ┆ 25]           │\n",
      "│                ┆ Freunde.       ┆ \"friends.\"]    ┆ \"Freunde.\"…   ┆               ┆               │\n",
      "└────────────────┴────────────────┴────────────────┴───────────────┴───────────────┴───────────────┘\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "import requests\n",
    "import io\n",
    "\n",
    "\n",
    "DE_TEST_FILE  = 'deu_test.json'\n",
    "DE_TRAIN_FILE = 'deu_training.json'\n",
    "FR_TEST_FILE  = 'fra_test.json'\n",
    "FR_TRAIN_FILE = 'fra_training.json'\n",
    "ES_TEST_FILE  = 'spa_test.json'\n",
    "ES_TRAIN_FILE = 'spa_training.json'\n",
    "\n",
    "def read_json_from_git(fname):\n",
    "    url = f'https://raw.githubusercontent.com/wortcook/LLMs-from-scratch/refs/heads/main/data/{fname}'\n",
    "    s = requests.get(url).content\n",
    "    \n",
    "    return pl.read_json(io.StringIO(s.decode('utf-8')))\n",
    "\n",
    "de_train  = read_json_from_git(DE_TRAIN_FILE)\n",
    "# print(de_train.head())\n",
    "\n",
    "\n",
    "#tokenize\n",
    "def tokenize(sentence):\n",
    "    return sentence.split()\n",
    "\n",
    "def build_vocab(vocab, df, col):\n",
    "    for sentence in df[col]:\n",
    "        for word in sentence:\n",
    "            if word not in vocab:\n",
    "                vocab[word] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "def index_tokens(tokens, vocab):\n",
    "    return [vocab[token] for token in tokens]\n",
    "\n",
    "#tokenize into a new column tokens\n",
    "de_train = de_train.with_columns([\n",
    "    pl.col(\"source\").map_elements(tokenize, return_dtype=pl.List(pl.Utf8)).alias(\"source_tokens\"),\n",
    "    pl.col(\"target\").map_elements(tokenize, return_dtype=pl.List(pl.Utf8)).alias(\"target_tokens\"),\n",
    "])\n",
    "\n",
    "# print(de_train.head())\n",
    "de_vocab = {\"\":0}\n",
    "de_vocab = build_vocab(de_vocab, de_train, \"source_tokens\")\n",
    "\n",
    "en_vocab = {\"\":0}\n",
    "en_vocab = build_vocab(en_vocab, de_train, \"target_tokens\")\n",
    "\n",
    "de_train = de_train.with_columns([\n",
    "    pl.col(\"source_tokens\").map_elements(lambda x: index_tokens(x, de_vocab), return_dtype=pl.List(pl.UInt16)).alias(\"source_indices\"),\n",
    "    pl.col(\"target_tokens\").map_elements(lambda x: index_tokens(x, en_vocab), return_dtype=pl.List(pl.UInt16)).alias(\"target_indices\"),\n",
    "])\n",
    "\n",
    "#find max length of source and target\n",
    "max_source_len = de_train[\"source_indices\"].list.len().max()\n",
    "max_target_len = de_train[\"target_indices\"].list.len().max()\n",
    "# max_target_len = de_train.with_columns([pl.col(\"target_indices\").list.len().max()])\n",
    "\n",
    "print(de_train.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (5, 10)\n",
      "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
      "│ source    ┆ target    ┆ source_to ┆ target_to ┆ … ┆ source_to ┆ target_to ┆ source_in ┆ target_i │\n",
      "│ ---       ┆ ---       ┆ kens      ┆ kens      ┆   ┆ kens_tran ┆ kens_tran ┆ dices_tra ┆ ndices_t │\n",
      "│ str       ┆ str       ┆ ---       ┆ ---       ┆   ┆ sformers  ┆ sformers  ┆ nsformers ┆ ransform │\n",
      "│           ┆           ┆ list[str] ┆ list[str] ┆   ┆ ---       ┆ ---       ┆ ---       ┆ ers      │\n",
      "│           ┆           ┆           ┆           ┆   ┆ list[str] ┆ list[str] ┆ list[u16] ┆ ---      │\n",
      "│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ list[u16 │\n",
      "│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ ]        │\n",
      "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
      "│ Tom       ┆ Tom hat   ┆ [\"Tom\",   ┆ [\"Tom\",   ┆ … ┆ [\"▁Tom\",  ┆ [\"▁Tom\",  ┆ [2136,    ┆ [2136,   │\n",
      "│ didn't    ┆ nicht mal ┆ \"didn't\", ┆ \"hat\", …  ┆   ┆ \"▁di\", …  ┆ \"▁hat\", … ┆ 1164, …   ┆ 109, …   │\n",
      "│ even      ┆ gelächelt ┆ …         ┆ \"gelächel ┆   ┆ \".\"]      ┆ \".\"]      ┆ 0]        ┆ 0]       │\n",
      "│ smile.    ┆ .         ┆ \"smile.\"] ┆ t.\"]      ┆   ┆           ┆           ┆           ┆          │\n",
      "│ Tom is a  ┆ Tom ist   ┆ [\"Tom\",   ┆ [\"Tom\",   ┆ … ┆ [\"▁Tom\",  ┆ [\"▁Tom\",  ┆ [2136,    ┆ [2136,   │\n",
      "│ really    ┆ ein       ┆ \"is\", …   ┆ \"ist\", …  ┆   ┆ \"▁is\", …  ┆ \"▁ist\", … ┆ 19, … 0]  ┆ 29, … 0] │\n",
      "│ bad       ┆ wirklich  ┆ \"singer.\" ┆ \"Sänger.\" ┆   ┆ \".\"]      ┆ \".\"]      ┆           ┆          │\n",
      "│ singer.   ┆ schlechte ┆ ]         ┆ ]         ┆   ┆           ┆           ┆           ┆          │\n",
      "│           ┆ …         ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│ I don't   ┆ Ich will  ┆ [\"I\",     ┆ [\"Ich\",   ┆ … ┆ [\"▁I\",    ┆ [\"▁Ich\",  ┆ [38, 17,  ┆ [105,    │\n",
      "│ want to   ┆ nicht     ┆ \"don't\",  ┆ \"will\", … ┆   ┆ \"▁\", …    ┆ \"▁will\",  ┆ … 0]      ┆ 73, … 0] │\n",
      "│ fail my   ┆ durch     ┆ …         ┆ \"fallen.\" ┆   ┆ \".\"]      ┆ … \".\"]    ┆           ┆          │\n",
      "│ exams.    ┆ meine     ┆ \"exams.\"] ┆ ]         ┆   ┆           ┆           ┆           ┆          │\n",
      "│           ┆ Prü…      ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│ There     ┆ Es gab    ┆ [\"There\", ┆ [\"Es\",    ┆ … ┆ [\"▁The\",  ┆ [\"▁Es\",   ┆ [36, 135, ┆ [149,    │\n",
      "│ were many ┆ viele Ver ┆ \"were\", … ┆ \"gab\", …  ┆   ┆ \"re\", …   ┆ \"▁gab\", … ┆ … 0]      ┆ 1647, …  │\n",
      "│ wounded.  ┆ letzte.   ┆ \"wounded. ┆ \"Verletzt ┆   ┆ \".\"]      ┆ \".\"]      ┆           ┆ 0]       │\n",
      "│           ┆           ┆ \"…        ┆ e.\"]      ┆   ┆           ┆           ┆           ┆          │\n",
      "│ We became ┆ Wir       ┆ [\"We\",    ┆ [\"Wir\",   ┆ … ┆ [\"▁We\",   ┆ [\"▁Wir\",  ┆ [157, 43, ┆ [182,    │\n",
      "│ very good ┆ wurden    ┆ \"became\", ┆ \"wurden\", ┆   ┆ \"▁be\", …  ┆ \"▁wurden\" ┆ … 0]      ┆ 298, …   │\n",
      "│ friends.  ┆ sehr gute ┆ … \"friend ┆ … \"Freund ┆   ┆ \".\"]      ┆ , … \".\"]  ┆           ┆ 0]       │\n",
      "│           ┆ Freunde.  ┆ s.\"]      ┆ e.\"…      ┆   ┆           ┆           ┆           ┆          │\n",
      "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-de-en\")\n",
    "\n",
    "def tokenize_with_transformers(sentence):\n",
    "    return tokenizer.tokenize(sentence)\n",
    "\n",
    "def encode_with_transformers(sentence):\n",
    "    return tokenizer.encode(sentence)\n",
    "\n",
    "de_train = de_train.with_columns([\n",
    "    pl.col(\"source\").map_elements(tokenize_with_transformers, return_dtype=pl.List(pl.Utf8)).alias(\"source_tokens_transformers\"),\n",
    "    pl.col(\"target\").map_elements(tokenize_with_transformers, return_dtype=pl.List(pl.Utf8)).alias(\"target_tokens_transformers\"),\n",
    "    pl.col(\"source\").map_elements(tokenizer.encode, return_dtype=pl.List(pl.UInt16)).alias(\"source_indices_transformers\"),\n",
    "    pl.col(\"target\").map_elements(tokenizer.encode, return_dtype=pl.List(pl.UInt16)).alias(\"target_indices_transformers\"),\n",
    "])\n",
    "\n",
    "print(de_train.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
